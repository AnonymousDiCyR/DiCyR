{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-39yq17uj because the default path (/home/david.bertoin/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from torch import nn\n",
    "from data_utils import load_synsigns, load_GTSRB\n",
    "from models import DomainAdaptationNetwork, get_simple_classifier, ProjectorNetwork\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from train import train_domain_adaptation\n",
    "from utils import test_network, plot_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_space_dim, conv_feat_size, nb_channels=3):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.latent_space_dim = latent_space_dim\n",
    "        self.conv_feat_size = conv_feat_size\n",
    "\n",
    "        self.deco_dense = nn.Sequential(\n",
    "            nn.Linear(in_features=latent_space_dim, out_features=1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(in_features=1024, out_features=np.prod(self.conv_feat_size)),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.deco_fetures = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.conv_feat_size[0], out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=5, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=nb_channels, kernel_size=5, padding=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z_share, z_spe):\n",
    "        z = torch.cat([z_share, z_spe], 1)\n",
    "        feat_encode = self.deco_dense(z)\n",
    "        feat_encode = feat_encode.view(-1, *self.conv_feat_size)\n",
    "        y = self.deco_fetures(feat_encode)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_space_dim, img_size, nb_channels=3):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.latent_space_dim = latent_space_dim\n",
    "        self.nb_channels = nb_channels\n",
    "\n",
    "        self.conv_feat = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.InstanceNorm2d(128),\n",
    "        )\n",
    "\n",
    "        self.conv_feat_size = self.conv_feat(torch.zeros(1, *img_size)).shape[1:]\n",
    "        self.dense_feature_size = np.prod(self.conv_feat_size)\n",
    "\n",
    "        self.dense_feat = nn.Linear(in_features=self.dense_feature_size, out_features=1024)\n",
    "        self.task_feat = nn.Linear(in_features=1024, out_features=latent_space_dim)\n",
    "        self.source_feat = nn.Linear(in_features=1024, out_features=latent_space_dim)\n",
    "        self.target_feat = nn.Linear(in_features=1024, out_features=latent_space_dim)\n",
    "\n",
    "    def forward(self, input_data, mode='all'):\n",
    "        if (input_data.shape[1] == 1) & (self.nb_channels == 3):\n",
    "            input_data = input_data.repeat(1, 3, 1, 1)\n",
    "        feat = self.conv_feat(input_data)\n",
    "        feat = feat.view(-1, self.dense_feature_size)\n",
    "        feat = F.relu(self.dense_feat(feat))\n",
    "        if mode == 'task':\n",
    "            z_task = F.relu(self.task_feat(feat))\n",
    "            return z_task\n",
    "        \n",
    "        elif mode == 'source':\n",
    "            z_source = F.relu(self.source_feat(feat))\n",
    "            return z_source\n",
    "        \n",
    "        elif mode == 'target':\n",
    "            z_target = F.relu(self.target_feat(feat))\n",
    "            return z_target\n",
    "        \n",
    "        else:\n",
    "            z_task = F.relu(self.task_feat(feat))\n",
    "            z_source = F.relu(self.source_feat(feat))\n",
    "            z_target = F.relu(self.target_feat(feat))\n",
    "            return z_task, z_source, z_target  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_classifier(latent_space_dim=1024):\n",
    "    return nn.Sequential(nn.Dropout2d(),\n",
    "                         nn.Linear(in_features=latent_space_dim, out_features=43),\n",
    "                         nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0 current target accuracy:46.62%:   3%|▎         | 1/30 [01:41<48:50, 101.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/30], loss:0.9654\n",
      "accuracy source: 85.21%\n",
      "accuracy target: 46.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:1 current target accuracy:84.53%:   7%|▋         | 2/30 [03:22<47:18, 101.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [2/30], loss:0.7472\n",
      "accuracy source: 98.71%\n",
      "accuracy target: 84.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:2 current target accuracy:92.34%:  10%|█         | 3/30 [05:04<45:41, 101.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [3/30], loss:0.6136\n",
      "accuracy source: 99.51%\n",
      "accuracy target: 92.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:3 current target accuracy:94.63%:  13%|█▎        | 4/30 [06:46<44:01, 101.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [4/30], loss:0.5687\n",
      "accuracy source: 99.52%\n",
      "accuracy target: 94.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:4 current target accuracy:95.07%:  17%|█▋        | 5/30 [08:27<42:20, 101.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [5/30], loss:0.5651\n",
      "accuracy source: 99.49%\n",
      "accuracy target: 95.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:5 current target accuracy:95.49%:  17%|█▋        | 5/30 [09:01<42:20, 101.63s/it]"
     ]
    }
   ],
   "source": [
    "target_loader = load_GTSRB(img_size=64, batch_size=128, shuffle=True, num_workers=4)\n",
    "source_loader = load_synsigns(img_size=64, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "learning_rate = 5e-4\n",
    "#epochs=10\n",
    "epochs=30\n",
    "\n",
    "encoder = Encoder(latent_space_dim=75, img_size=(3,64,64), nb_channels=3)\n",
    "conv_feat_size = encoder.conv_feat_size\n",
    "decoder_source = Decoder(latent_space_dim=150, conv_feat_size=conv_feat_size, nb_channels=3)\n",
    "decoder_target = Decoder(latent_space_dim=150, conv_feat_size=conv_feat_size, nb_channels=3)\n",
    "classifier = get_simple_classifier(latent_space_dim=75)\n",
    "model = DomainAdaptationNetwork(encoder, decoder_source, decoder_target, classifier).cuda()\n",
    "random_projector = ProjectorNetwork(latent_dim=75).cuda()\n",
    "betas = np.ones(30) * 10\n",
    "betas[:10] = np.linspace(0, 10, 10)\n",
    "#betas = np.linspace(0, 5, 30)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=0.001)\n",
    "\n",
    "train_domain_adaptation(model, optimizer, random_projector, source_loader, target_loader, betas=betas,\n",
    "                                            epochs=epochs, alpha=1, gamma=0.5, delta=0.5, show_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_network(model, target_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = np.ones(30) * 10\n",
    "#betas = np.linspace(0, 5, 30)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, weight_decay=0.001)\n",
    "\n",
    "train_domain_adaptation(model, optimizer, random_projector, source_loader, target_loader, betas=betas,\n",
    "                                            epochs=epochs, alpha=1, gamma=0.5, delta=0.5, show_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_network(model, target_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_target_cross_domain_swapping(model, source_train_loader, target_train_loader):\n",
    "    X, _ = next(iter(target_train_loader))\n",
    "    y, _, (z_share, z_spe),  _, _ = model.forward_t(X.cuda())\n",
    "    X2, _ = next(iter(source_train_loader))\n",
    "    _, _, (z_share, _),  _, _ = model.forward_t(X2.cuda())\n",
    "    #blank\n",
    "    plt.subplot(1,6,1)\n",
    "    plt.imshow(torch.ones((32,32,3)))\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    #styles\n",
    "    for i in range(5):\n",
    "        plt.subplot(1,6,i+2)\n",
    "        plt.imshow(X[i].cpu().detach().permute(1, 2, 0))\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "\n",
    "    for j in range(10, 20):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,6,1)\n",
    "        plt.imshow(X2[j].cpu().detach().permute(1, 2, 0))\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        z_x = torch.zeros_like(z_share)\n",
    "        z_x[:] = z_share[j]\n",
    "        y2  = model.decoder_target(z_x, z_spe)\n",
    "        for i in range(5):\n",
    "            plt.subplot(1,6,i+2)\n",
    "            plt.imshow(y2[i].cpu().detach().permute(1, 2, 0))\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_target_cross_domain_swapping(model, source_loader, target_loader)\n",
    "#plot_tsne(model, source_train_loader, target_train_loader, 128, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
